












ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 













 Search





















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 














Capacity of Neural Networks
Home>Seminars>Capacity of Neural Networks















Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					











































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.



































ACO Center @ UCI • Privacy Policy • © 2022 UC Regents 

















ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 













 Search





















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 














Capacity of Neural Networks
Home>Seminars>Capacity of Neural Networks















Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					











































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.



































ACO Center @ UCI • Privacy Policy • © 2022 UC Regents 














ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 













 Search





















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 


















ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 













 Search





















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

















Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 















ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 













 Search
















ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 













 Search














ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 








ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 






ACO Center @ UCI  




Algorithms, Combinatorics and Optimization 




ACO Center @ UCI  

ACO Center @ UCI  


Algorithms, Combinatorics and Optimization 



Algorithms, Combinatorics and Optimization 

Algorithms, Combinatorics and Optimization Algorithms, Combinatorics and Optimization







 Search















 Search













 Search











 Search









 Search






 Search



 Search
Search







Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 













Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 











Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 









Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 







Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 





Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 



Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 
HomePeopleSeminar SeriesSynergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation

ICSMathBusiness SchoolIMBSEECSCenter for Algorithms and Theory of ComputationNewsContact
 Menu
MenuHomePeopleSeminar SeriesSynergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation

ICSMathBusiness SchoolIMBSEECSCenter for Algorithms and Theory of ComputationNewsContact







Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 













Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 











Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 









Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 







Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 





Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 



Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 

 Menu


Home
People
Seminar Series
Synergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation


News
Contact
 
HomePeopleSeminar SeriesSynergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation

ICSMathBusiness SchoolIMBSEECSCenter for Algorithms and Theory of ComputationNewsContact
 Menu
MenuHomePeopleSeminar SeriesSynergistic Units at UCI

ICS
Math
Business School
IMBS
EECS
Center for Algorithms and Theory of Computation

ICSMathBusiness SchoolIMBSEECSCenter for Algorithms and Theory of ComputationNewsContact
Capacity of Neural Networks
Home>Seminars>Capacity of Neural Networks
Home>HomeHome>Seminars>Seminars>Capacity of Neural NetworksCapacity of Neural Networks












Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					











































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.







































Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					











































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.





























Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					











































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.



























Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					











































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.

























Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					











































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.






















Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					












Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					










Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					








Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					






Capacity of Neural Networks 




Date: November 29, 2018					





Time: 2:00 pm					





Room: DBH 4011					





Speaker: Roman Vershynin 					





				 (Math, UCI)					




Capacity of Neural Networks 

Capacity of Neural Networks 


Date: November 29, 2018					



Date: November 29, 2018					

Date: November 29, 2018					


Time: 2:00 pm					



Time: 2:00 pm					

Time: 2:00 pm					


Room: DBH 4011					



Room: DBH 4011					

Room: DBH 4011					


Speaker: Roman Vershynin 					



Speaker: Roman Vershynin 					

Speaker: Roman Vershynin 					


				 (Math, UCI)					



				 (Math, UCI)					

				 (Math, UCI)					































































































































Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.




















Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.


















Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.
















Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.














Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.












Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.




Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.


Abstract: Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.
Deep learning is a rapidly developing area of machine learning, which uses artificial neural networks to perform learning tasks. Although mathematical description of neural networks is simple, theoretical justification for the spectacular performance of deep learning remains elusive. Even the most basic questions about remain open. For example, how many different functions can a neural network compute? Jointly with Pierre Baldi (UCI CS) we discovered a general capacity formula for all fully connected networks with the threshold activation function. The formula predicts, counterintuitively, that shallow networks have greater capacity than deep ones. So, the mystery remains.





















ACO Center @ UCI • Privacy Policy • © 2022 UC Regents 






ACO Center @ UCI • Privacy Policy • © 2022 UC Regents 




ACO Center @ UCI • Privacy Policy • © 2022 UC Regents 


ACO Center @ UCI • Privacy Policy • © 2022 UC Regents 

Close Menu

Close Menu








Open toolbar

 

Open toolbar

Accessibility Tools



Increase Text 



Decrease Text 



Grayscale 



High Contrast 



Negative Contrast 



Light Background 



Links Underline 



Readable Font 




Reset





Accessibility Tools



Increase Text 



Decrease Text 



Grayscale 



High Contrast 



Negative Contrast 



Light Background 



Links Underline 



Readable Font 




Reset



Accessibility Tools

Increase Text 
Increase Text

Decrease Text 
Decrease Text

Grayscale 
Grayscale

High Contrast 
High Contrast

Negative Contrast 
Negative Contrast

Light Background 
Light Background

Links Underline 
Links Underline

Readable Font 
Readable Font


Reset

Reset